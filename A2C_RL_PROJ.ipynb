{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45d7b387",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "491ee237",
   "metadata": {},
   "outputs": [],
   "source": [
    "class a2c(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_dim,layer_dim,num_act):\n",
    "        super(a2c, self).__init__()\n",
    "        \n",
    "        #actor setup:\n",
    "\n",
    "        self.afc1 = nn.Linear(4, layer_dim)\n",
    "        self.aact = nn.ReLU()\n",
    "        self.afc2 = nn.Linear(layer_dim,num_act)\n",
    "        \n",
    "        #critic setup:\n",
    "        self.cfc1 = nn.Linear(4, layer_dim)\n",
    "        self.cact = nn.ReLU()\n",
    "        self.cfc2 = nn.Linear(layer_dim,1) \n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        #get the distribution from the actor\n",
    "        actor_dist = self.afc1(x)\n",
    "        actor_dist = self.aact(actor_dist)\n",
    "        actor_dist = self.afc2(actor_dist) \n",
    "        actor_dist = F.softmax(actor_dist,dim=1)\n",
    "        \n",
    "        #get the critic value \n",
    "        critic_val = self.cfc1(x)\n",
    "        critic_val = self.cact(critic_val)\n",
    "        critic_val = self.cfc2(critic_val)\n",
    "        \n",
    "\n",
    "        return actor_dist , critic_val.item() \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0c7b4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline -- > real use this\n",
    "def run_A2C(num_eps,n_step):\n",
    "    \n",
    "    #CartpoleEnvironment:\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    num_inputs = env.observation_space.shape[0]\n",
    "    num_outputs = env.action_space.n\n",
    "    \n",
    "    #WindyGridWordEnvironment:\n",
    "#     env = WindyGridWorld(enable_king_move=False,\n",
    "#                      enable_no_move=False)\n",
    "#     num_inputs = 2\n",
    "#     num_outputs = len(env.action_space)\n",
    "    \n",
    "    #Four room environment\n",
    "#     env = FourRooms()\n",
    "#     num_inputs = len(env.state_space)\n",
    "#     num_outputs = len(env.action_space)\n",
    "\n",
    "    #Blocking Maze environmnet:\n",
    "\n",
    "#     env = BlockingMaze()\n",
    "#     num_inputs = 2\n",
    "#     num_outputs = len(env.action_space)\n",
    "#\n",
    "\n",
    "    actor_critic = a2c(num_inputs,128,num_outputs)\n",
    "    optimizer = optim.Adam(actor_critic.parameters(),lr=3e-4)\n",
    "    \n",
    "    #lists to keep track of values over all episodes\n",
    "    episodic_total_rewards = []\n",
    "    continous_total_rewards=[]\n",
    "    entropy_term = []\n",
    "    cumalative_reward = 0\n",
    "    \n",
    "    for j in tqdm(range(num_eps)):\n",
    "        \n",
    "        total_rewards = []\n",
    "        actor_log_prob = []\n",
    "        critic_val = []\n",
    "\n",
    "        state = env.reset()\n",
    "        \n",
    "        for steps in range(n_step):\n",
    "            \n",
    "            if steps == 0:\n",
    "                state = (torch.tensor((state[0])).float().unsqueeze(0))\n",
    "            else:\n",
    "                state = (torch.tensor((state)).float().unsqueeze(0))\n",
    "            \n",
    "            actor_policy, c_val = actor_critic(state)\n",
    "            \n",
    "            #get the action from the policy and the subseqent log of the policy for loss computation in the future\n",
    "            action = actor_policy.multinomial(num_samples=1, replacement=True).item()\n",
    "            actor_policy_log = (torch.log(actor_policy)[0][action])\n",
    "            actor_log_prob.append(actor_policy_log)\n",
    "            \n",
    "            #compute entropy - it is summed over all values for the whole training cycle \n",
    "            entropy = -np.sum(np.mean(actor_policy.detach().numpy()) * np.log(actor_policy.detach().numpy()))\n",
    "            entropy_term.append(entropy)\n",
    "            \n",
    "            #keep trrack of the critic value\n",
    "            critic_val.append(c_val)\n",
    "\n",
    "            #cartpole\n",
    "            new_state, reward, done,d, _ = env.step(action)\n",
    "            \n",
    "            #windy\n",
    "            #new_state, reward, done= env.step( list(env.action_space.keys())[action]) \n",
    "#             rewards.append(reward)\n",
    "            \n",
    "            #block\n",
    "            #new_state, reward, done,d , _= env.step( list(env.action_space.keys())[action]) \n",
    "#             rewards.append(reward)\n",
    "            \n",
    "            state = new_state\n",
    "            total_rewards.append(reward)\n",
    "            cumalative_reward+=reward\n",
    "            continous_total_rewards.append(cumalative_reward)\n",
    "            \n",
    "            if done:\n",
    "                #if the episode is done then terminate and dont go on to optimize\n",
    "                new_state = (torch.tensor((new_state)).float().unsqueeze(0))\n",
    "                actor_target, critic_ret = actor_critic(new_state)\n",
    "                actor_target = (actor_target.detach()[0][0].item())\n",
    "                break\n",
    "        \n",
    "        episodic_total_rewards.append(np.sum(total_rewards))\n",
    "        \n",
    "        targets = compute_targets(total_rewards,actor_target)\n",
    "        loss = compute_loss(critic_val, targets, actor_log_prob, np.sum(entropy_term))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    #for episodic returns\n",
    "    return episodic_total_rewards \n",
    "    #for time step returns\n",
    "#     return continous_total_rewards for continous\n",
    "\n",
    "def compute_loss(cur_vals, state_vals,log_probs,entropy):\n",
    "    \n",
    "    cur_vals_tensor = torch.FloatTensor(cur_vals)\n",
    "    state_val_tensor = torch.FloatTensor(state_vals)\n",
    "    log_probs_tensor = torch.stack(log_probs)\n",
    "        \n",
    "    adv_func = state_val_tensor - cur_vals_tensor\n",
    "    loss = ((-log_probs_tensor*adv_func).mean()) + (0.5*adv_func.pow(2).mean()) + (0.001*entropy)\n",
    "    return loss\n",
    "\n",
    "def compute_targets(rewards,targets):\n",
    "    target_list = []\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        targets = rewards[i] + (0.99*targets)\n",
    "        target_list.insert(0,targets)\n",
    "    return target_list\n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c356a39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Windy GridWorld Env\"\"\"\n",
    "class WindyGridWorld(object):\n",
    "    def __init__(self, enable_king_move=False, enable_no_move=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            enable_king_move (bool): If True, using King's movement. Otherwise, using the original action space.\n",
    "            enable_no_move (bool): If True, adding no movement under the condition of enable_king_move = True.\n",
    "        \n",
    "        Note: for different setup, we have the following action spaces:\n",
    "            - Original: [\"up\", \"down\", \"left\", \"right\"];\n",
    "            - King's move: [\"up\", \"down\", \"left\", \"right\", \"up-left\", \"up-right\", \"down-left\", \"down-right\"];\n",
    "            - King's move + no move: [\"up\", \"down\", \"left\", \"right\", \"up-left\", \"up-right\", \"down-left\", \"down-right\", \"stay\"]\n",
    "        \"\"\"\n",
    "        \n",
    "        # define the grid space\n",
    "        self.grid = np.zeros((7, 10))\n",
    "\n",
    "        # define the state space\n",
    "        self.state_space = [[r, c] for r, c in zip(np.where(self.grid == 0.0)[0],\n",
    "                                                   np.where(self.grid == 0.0)[1])]\n",
    "\n",
    "        # define the start state\n",
    "        self.start_state = [3, 0]\n",
    "\n",
    "        # define the goal state\n",
    "        self.goal_state = [3, 7]\n",
    "\n",
    "        # define the wind\n",
    "        self.wind = np.array([0, 0, 0, 1, 1, 1, 2, 2, 1, 0], dtype=int)\n",
    "\n",
    "        # define the action space\n",
    "        if enable_king_move:\n",
    "            # add King's move actions\n",
    "            if enable_no_move:\n",
    "                self.action_space = {\n",
    "                    \"up\": np.array([-1, 0]),\n",
    "                    \"down\": np.array([1, 0]),\n",
    "                    \"left\": np.array([0, -1]),\n",
    "                    \"right\": np.array([0, 1]),\n",
    "                    \"up-right\": np.array([-1, 1]),  # add up-right\n",
    "                    \"up-left\": np.array([-1, -1]),  # add up-left\n",
    "                    \"down-right\": np.array([1, 1]),  # add down-right\n",
    "                    \"down-left\": np.array([1, -1]),  # add down-left\n",
    "                    \"stay\": np.array([0, 0])  # add no move action\n",
    "                }\n",
    "            else:\n",
    "                # add King's move actions + one no movement action\n",
    "                self.action_space = {\n",
    "                    \"up\": np.array([-1, 0]),\n",
    "                    \"down\": np.array([1, 0]),\n",
    "                    \"left\": np.array([0, -1]),\n",
    "                    \"right\": np.array([0, 1]),\n",
    "                    \"up-right\": np.array([-1, 1]),  # add up-right\n",
    "                    \"up-left\": np.array([-1, -1]),  # add up-left\n",
    "                    \"down-right\": np.array([1, 1]),  # add down-right\n",
    "                    \"down-left\": np.array([1, -1])  # add down-left\n",
    "                }\n",
    "        else:\n",
    "            # normal actions\n",
    "            self.action_space = {\n",
    "                \"up\": np.array([-1, 0]),\n",
    "                \"down\": np.array([1, 0]),\n",
    "                \"left\": np.array([0, -1]),\n",
    "                \"right\": np.array([0, 1])\n",
    "            }\n",
    "\n",
    "        # track the current state, time step, and action\n",
    "        self.state = None\n",
    "        self.t = None\n",
    "        self.act = None\n",
    "\n",
    "    def reset(self):\n",
    "        # reset the agent to the start state\n",
    "        self.state = self.start_state\n",
    "        # reset the time step tracker\n",
    "        self.t = 0\n",
    "        # reset the action tracker\n",
    "        self.act = None\n",
    "        # reset the terminal flag\n",
    "        terminated = False\n",
    "        return self.state, terminated\n",
    "\n",
    "    def step(self, act):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            act (string): a string variable indicating the action.\n",
    "        \"\"\"\n",
    "        # obtain the state array\n",
    "        s_arr = np.array(self.state)\n",
    "\n",
    "        # obtain the action array\n",
    "        act_arr = self.action_space[act.lower()]\n",
    "\n",
    "        # obtain the wind array\n",
    "        wind_arr = -1 * np.array([self.wind[self.state[1]], 0], dtype=int)\n",
    "\n",
    "        # compute the next state\n",
    "        next_s_arr = np.clip(s_arr + act_arr + wind_arr,\n",
    "                             a_min=np.array([0, 0]),\n",
    "                             a_max=np.array([self.grid.shape[0]-1, self.grid.shape[1]-1]))\n",
    "\n",
    "        # compute the reward\n",
    "        reward = 1 if next_s_arr.tolist() == self.goal_state else 0\n",
    "\n",
    "        # check the termination\n",
    "        terminated = True if reward == 1 else False\n",
    "\n",
    "        # update the tracking variables\n",
    "        self.state = next_s_arr.tolist()\n",
    "        self.t += 1\n",
    "        self.act = act\n",
    "\n",
    "        return self.state, reward, terminated\n",
    "\n",
    "    def render(self):\n",
    "        # plot the agent and the goal\n",
    "        # agent = 1\n",
    "        # goal = 2\n",
    "        plot_arr = self.grid.copy()\n",
    "        plot_arr[self.state[0], self.state[1]] = 1.0\n",
    "        plot_arr[self.goal_state[0], self.goal_state[1]] = 2.0\n",
    "        plt.clf()\n",
    "        fig, arr = plt.subplots(1, 1)\n",
    "        arr.set_title(f\"state={self.state}, act={self.act}\")\n",
    "        arr.imshow(plot_arr)\n",
    "        plt.show(block=False)\n",
    "        plt.pause(1)\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed0e74ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Four Rooms Environment Implementation\n",
    "\n",
    "\"\"\"\n",
    "class FourRooms(object):\n",
    "    def __init__(self, max_time_steps=459):\n",
    "        # We define the grid for the Four Rooms domain\n",
    "        self.grid = np.array([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                              [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1],\n",
    "                              [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]])\n",
    "\n",
    "        # We define the observation space consisting of all empty cells\n",
    "        # Note: We have to flip the coordinates from (row_idx, column_idx) -> (x, y),\n",
    "        # where x = column_idx, y = 10 - row_idx\n",
    "        self.observation_space = np.argwhere(self.grid == 0.0).tolist()  # Fine all empty cells\n",
    "        self.observation_space = self.arr_coords_to_four_room_coords(self.observation_space)\n",
    "\n",
    "        # We define the action space\n",
    "        self.action_space = {'up': np.array([0, 1]),\n",
    "                             'down': np.array([0, -1]),\n",
    "                             'left': np.array([-1, 0]),\n",
    "                             'right': np.array([1, 0])}\n",
    "        self.action_names = ['up', 'down', 'left', 'right']\n",
    "\n",
    "        # We define the start location\n",
    "        self.start_location = [0, 0]\n",
    "\n",
    "        # We define the goal location\n",
    "        self.goal_location = [10, 10]\n",
    "\n",
    "        # We find all wall cells\n",
    "        self.walls = np.argwhere(self.grid == 1.0).tolist()  # find all wall cells\n",
    "        self.walls = self.arr_coords_to_four_room_coords(self.walls)  # convert to Four Rooms coordinates\n",
    "\n",
    "        # This is an episodic task, we define a timeout: maximal time steps = 459\n",
    "        self.max_time_steps = max_time_steps\n",
    "\n",
    "        # We define other useful variables\n",
    "        self.agent_location = None  # track the agent's location in one episode.\n",
    "        self.action = None  # track the agent's action\n",
    "        self.t = 0  # track the current time step in one episode\n",
    "\n",
    "    @staticmethod\n",
    "    def arr_coords_to_four_room_coords(arr_coords_list):\n",
    "        \"\"\"\n",
    "        Function converts the array coordinates to the Four Rooms coordinates (i.e, The origin locates at bottom left).\n",
    "        E.g., The coordinates (0, 0) in the numpy array is mapped to (0, 10) in the Four Rooms coordinates.\n",
    "        Args:\n",
    "            arr_coords_list (list): a list variable consists of tuples of locations in the numpy array\n",
    "\n",
    "        Return:\n",
    "            four_room_coords_list (list): a list variable consists of tuples of converted locations in the\n",
    "                                          Four Rooms environment.\n",
    "        \"\"\"\n",
    "        # Note: We have to flip the coordinates from (row_idx, column_idx) -> (x, y),\n",
    "        # where x = column_idx, y = 10 - row_idx\n",
    "        four_room_coords_list = [(column_idx, 10 - row_idx) for (row_idx, column_idx) in arr_coords_list]\n",
    "        return four_room_coords_list\n",
    "\n",
    "    def reset(self):\n",
    "        # We reset the agent's location to the start location\n",
    "        self.agent_location = self.start_location\n",
    "\n",
    "        # We reset the timeout tracker to be 0\n",
    "        self.t = 0\n",
    "\n",
    "        # We set the information\n",
    "        info = {}\n",
    "        return self.agent_location, info\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            action (string): a string variable (i.e., \"UP\"). All feasible values are [\"up\", \"down\", \"left\", \"right\"].\n",
    "        \"\"\"\n",
    "        # With probability 0.8, the agent takes the correct direction.\n",
    "        # With probability 0.2, the agent takes one of the two perpendicular actions.\n",
    "        # For example, if the correct action is \"LEFT\", then\n",
    "        #     - With probability 0.8, the agent takes action \"LEFT\";\n",
    "        #     - With probability 0.1, the agent takes action \"UP\";\n",
    "        #     - With probability 0.1, the agent takes action \"DOWN\".\n",
    "        if np.random.uniform() < 0.2:\n",
    "            if action == \"left\" or action == \"right\":\n",
    "                action = np.random.choice([\"up\", \"down\"], 1)[0]\n",
    "            else:\n",
    "                action = np.random.choice([\"right\", \"left\"], 1)[0]\n",
    "\n",
    "        # Convert the agent's location to array\n",
    "        loc_arr = np.array(self.agent_location)\n",
    "\n",
    "        # Convert the action name to movement array\n",
    "        act_arr = self.action_space[action]\n",
    "\n",
    "        # Compute the agent's next location\n",
    "        next_agent_location = np.clip(loc_arr + act_arr,\n",
    "                                      a_min=np.array([0, 0]),\n",
    "                                      a_max=np.array([10, 10])).tolist()\n",
    "\n",
    "        # Check if the agent crashes into walls, it stays at the current location.\n",
    "        if tuple(next_agent_location) in self.walls:\n",
    "            next_agent_location = self.agent_location\n",
    "\n",
    "        # Compute the reward\n",
    "        reward = 1.0 if next_agent_location == self.goal_location else 0.0\n",
    "\n",
    "        # Check the termination\n",
    "        # If the agent reaches the goal, reward = 1, done = True\n",
    "        # If the time steps reaches the maximal number, reward = 0, done = True.\n",
    "        if reward == 1.0 or self.t == self.max_time_steps:\n",
    "            terminated = True\n",
    "        else:\n",
    "            terminated = False\n",
    "\n",
    "        # Update the agent's location, action and time step trackers\n",
    "        self.agent_location = next_agent_location\n",
    "        self.action = action\n",
    "        self.t += 1\n",
    "\n",
    "        return next_agent_location, reward, terminated, False, {}\n",
    "\n",
    "    def render(self):\n",
    "        # plot the agent and the goal\n",
    "        # empty cell = 0\n",
    "        # wall cell = 1\n",
    "        # agent cell = 2\n",
    "        # goal cell = 3\n",
    "        plot_arr = self.grid.copy()\n",
    "        plot_arr[10 - self.agent_location[1], self.agent_location[0]] = 2\n",
    "        plot_arr[10 - self.goal_location[1], self.goal_location[0]] = 3\n",
    "        plt.clf()\n",
    "        plt.title(f\"state={self.agent_location}, act={self.action}\")\n",
    "        plt.imshow(plot_arr)\n",
    "        plt.show(block=False)\n",
    "        plt.pause(0.1)\n",
    "\n",
    "    @staticmethod\n",
    "    def test():\n",
    "        my_env = FourRooms()\n",
    "        state, _ = my_env.reset()\n",
    "\n",
    "        for _ in range(100):\n",
    "            action = np.random.choice(list(my_env.action_space.keys()), 1)[0]\n",
    "\n",
    "            next_state, reward, done, _, _ = my_env.step(action)\n",
    "            my_env.render()\n",
    "\n",
    "            if done:\n",
    "                state, _ = my_env.reset()\n",
    "            else:\n",
    "                state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "590f8a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3000/3000 [02:26<00:00, 20.53it/s]\n",
      "100%|███████████████████████████████████████| 3000/3000 [02:22<00:00, 21.01it/s]\n",
      "100%|███████████████████████████████████████| 3000/3000 [02:16<00:00, 22.02it/s]\n",
      "100%|███████████████████████████████████████| 3000/3000 [01:52<00:00, 26.63it/s]\n",
      "100%|███████████████████████████████████████| 3000/3000 [02:05<00:00, 23.98it/s]\n"
     ]
    }
   ],
   "source": [
    "#general code to run \n",
    "avg_rewards = [ ]\n",
    "n_iterations = 5\n",
    "for i in (range(n_iterations)):\n",
    "    test = run_A2C(3000,10000)\n",
    "    avg_rewards.append(np.array(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff704ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#not the results used in the paper just a sample run\n",
    "plt.plot(range(len(np.array(avg_rewards).mean(axis=0))),np.array(avg_rewards).mean(axis=0))\n",
    "plt.title('A2C Episodic Rewards: Cartpole')\n",
    "plt.savefig('RL Preliminary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b93c28ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7831330",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
